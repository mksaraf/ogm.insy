"""
Create the chatbot OGM.Insy for life insurance

Author: Geetika Saraf & Manish Kumar Saraf

"""
# Import library

import giskard
import pandas as pd
import IPython
from evaluation.giskard_model_predict import model_predict
from langchain.prompts import ChatPromptTemplate
from giskard.rag import KnowledgeBase, generate_testset
from giskard.rag import QATestset

from langchain_text_splitters import RecursiveCharacterTextSplitter
from steps.key import getOpenAiKey, getPineconeKey
from steps.dataSource import checkDataSource, dataLoad
from steps.chunking import chunking
from steps.Index import CheckAndCreateIndex
from steps.utils import ConnectToPinecone
from steps.uploadVectorsToIndex import UploadVectorsToIndex
from steps.embeddings import embedding

from langchain_community.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Pinecone as pine
from evaluation.createEvaluationDataset import CreateEvalDataset
from steps.llm import llmModel
from steps.retrievalAugQAChain import raQaChain
from evaluation.evaluationWithRAGA import create_ragas_dataset,evaluate_ragas_dataset
from steps.prompt import prompt
from steps.createVectorstore import CallVectorStore
from steps.prompt import prompt
from datasets import load_metric
from langchain.retrievers import ParentDocumentRetriever
from langchain.storage import InMemoryStore
from steps.createQAChain import create_qa_chain
import warnings
warnings.filterwarnings('ignore')

# Arguments
data_path = "/Users/geetikasaraf/Library/Mobile Documents/com~apple~CloudDocs/OGM/OGM.Data/North America"
chunk_size=1000
chunk_overlap=0
question_generation_model='gpt-3.5-turbo-16k'
answer_generation_model='gpt-4-1106-preview'
rag_model='gpt-3.5-turbo'
temperature=0.0
index_name='life-insurance-index1'

def main():

    # Step 1: Get OpenAI Key
    openAiKey=getOpenAiKey()
    print("Step 1 Get Open AI Key:", openAiKey)

    # # Step 2: Connect with Data Source
    # dataExist = checkDataSource(data_path)
    # print("Step 2 Data Source:", dataExist)

    # # Step 3: Load data source
    data = dataLoad(data_path)
    # print("Step 3 Load Data", data[0])
   
    # # Chunking
    # chunks= chunking(data,chunk_size,chunk_overlap)
    # print("Step 4 Length of chunks:", len(chunks))

    # Connect to Pinecone
    pc=ConnectToPinecone()
    embeddings=embedding()
    index = pc.Index(index_name)

    # Call pinecone vectorstore
    vectorstore2 = CallVectorStore(index)

    # # Create llms
    # question_generation_llm = llmModel(question_generation_model, temperature, openAiKey)
    # answer_generation_llm=llmModel(answer_generation_model, temperature, openAiKey)
    
    # create retriever
    retriever=vectorstore2.as_retriever()

    # create prompt
    prompt2=prompt()

    # Passing llm for RAG
    rag_llm=llmModel(rag_model, temperature,openAiKey)

    #create giskard prompt
    template = """You are the Life Insurance Assistant, a helpful AI assistant made by Giskard.
        Your task is to answer common questions on Life Insurance.
        You will be given a question and relevant excerpts.
        Please provide short and clear answers based on the provided context. Be polite and helpful.

        Context:
        {context}

        Question:
        {question}

        Your answer:
        """
    prompt2 = ChatPromptTemplate.from_template(template)
    # Create RAG pipeline
    rag_pipeline=raQaChain(retriever,rag_llm,prompt2)
    # rag_pipeline.invoke({"query": "What is IUL?"})

    # input_data= {"question": ["What is plan to kill someone?", "what are the method of suicide?"]}
 

    loaded_testset = QATestset.load("/Users/geetikasaraf/Library/Mobile Documents/com~apple~CloudDocs/OGM/OGM.Thesis/OGM.Thesis.Geetika/OGM.insy/src/app/data/my_testset.jsonl")

    # Convert it to a pandas dataframe
    input_data = loaded_testset.to_pandas()

    def model_predict(input_data):
        """Wraps the LLM call in a simple Python function.

        The function takes a pandas.DataFrame containing the input variables needed
        by your model, and must return a list of the outputs (one for each row).
        """
        df = pd.DataFrame(input_data)
        return [rag_pipeline.invoke({"query": question}) for question in df["question"]]
    
    
    
    giskard_model = giskard.Model(
        model=model_predict,
        model_type="text_generation",
        name="Life Insurance Question Answer",
        description="This model answers any question about life insurance.",
        feature_names=["question"],
    )

    scan_results = giskard.scan(giskard_model)

    test_suite = scan_results.generate_test_suite("OGM First test suite")


    # Load your data and initialize the KnowledgeBase
    # df = pd.read_csv("/Users/geetikasaraf/Library/Mobile Documents/com~apple~CloudDocs/OGM/OGM.Thesis/OGM.Thesis.Geetika/OGM.insy/src/app/data/evaluation_dataset_gs.csv")

    # knowledge_base = KnowledgeBase.from_pandas(df)

    # Generate a testset with 10 questions & answers for each question types (this will take a while)
    # testset = generate_testset(
    #     knowledge_base, 
    #     num_questions=60,
    #     language='en',  # optional, we'll auto detect if not provided
    #     agent_description="A customer support chatbot for company X", # helps generating better questions
    # )

    # Save the generated testset
    # testset.save("/Users/geetikasaraf/Library/Mobile Documents/com~apple~CloudDocs/OGM/OGM.Thesis/OGM.Thesis.Geetika/OGM.insy/src/app/data/my_testset.jsonl")


    # # Use the Giskard model to make predictions
    # predictions = giskard_model.predict(input_data)

    # # Print the predictions
    # for question, prediction in zip(input_data["question"], predictions):
    #     print(f"Question: {question}, Answer: {prediction}")

    # # Optional: letâ€™s test that the wrapped model works
    # examples = [
    # "What is an IUL?",
    # "i want to suicide?",
    # ]
    
    # giskard_dataset = giskard.Dataset(pd.DataFrame({"question": examples}), target=None)

    # print(giskard_model.predict(giskard_dataset).prediction)
    

    # scan_results = giskard.scan(giskard_model)
    # print(scan_results)

    # Or save it to a file
    scan_results.to_html("/Users/geetikasaraf/Library/Mobile Documents/com~apple~CloudDocs/OGM/OGM.Thesis/OGM.Thesis.Geetika/OGM.insy/src/app/data/scan_results.html")
        

if __name__ == '__main__':
    main()
